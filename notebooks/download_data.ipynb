{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download datasets for mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import ee\n",
    "from geojson import Point, Feature, FeatureCollection, dump\n",
    "import geopandas\n",
    "from keplergl import KeplerGl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import box, shape, Point, Polygon\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sys.path.append('../')\n",
    "from scripts.get_s2_data_ee import get_history, get_history_polygon, get_pixel_vectors\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel 2 band descriptions\n",
    "band_descriptions = {\n",
    "    'B1': 'Aerosols, 442nm',\n",
    "    'B2': 'Blue, 492nm',\n",
    "    'B3': 'Green, 559nm',\n",
    "    'B4': 'Red, 665nm',\n",
    "    'B5': 'Red Edge 1, 704nm',\n",
    "    'B6': 'Red Edge 2, 739nm',\n",
    "    'B7': 'Red Edge 3, 779nm',\n",
    "    'B8': 'NIR, 833nm',\n",
    "    'B8A': 'Red Edge 4, 864nm',\n",
    "    'B9': 'Water Vapor, 943nm',\n",
    "    'B11': 'SWIR 1, 1610nm',\n",
    "    'B12': 'SWIR 2, 2186nm'\n",
    "}\n",
    "\n",
    "rect_width = 0.01\n",
    "scale = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_stack(patch_history):\n",
    "    mean_stack = []\n",
    "    dates = list(patch_history.keys())\n",
    "    for site in patch_history[dates[0]]:\n",
    "        img_stack = []\n",
    "        for date in dates:\n",
    "            spectral_stack = []\n",
    "            band_shapes = [np.shape(patch_history[date][site][band])[0] for band in band_descriptions]\n",
    "            if np.array(band_shapes).all() > 0:\n",
    "                for band in band_descriptions:\n",
    "                    spectral_stack.append(patch_history[date][site][band])\n",
    "                img_stack.append(np.rollaxis(np.array(spectral_stack), 0, 3))\n",
    "\n",
    "        masked_img = []\n",
    "        for img in img_stack:\n",
    "            masked_img.append(np.ma.masked_where(img < 0, img))\n",
    "        \n",
    "        masked_mean = np.ma.mean(masked_img, axis=0)\n",
    "        \n",
    "        num_cloudy_pixels = np.sum(masked_mean.mask)\n",
    "        cloud_fraction = num_cloudy_pixels / np.size(masked_mean)\n",
    "        \n",
    "        print(\"Cloud Fraction\", cloud_fraction)\n",
    "        if cloud_fraction < 0.2:\n",
    "            mean_stack.append(masked_mean.data)\n",
    "    return mean_stack\n",
    "\n",
    "def normalize(x):\n",
    "    return (np.array(x)) / (3000)\n",
    "\n",
    "def create_sampling_grid(min_lon, max_lon, min_lat, max_lat, steps_lon, steps_lat):\n",
    "    lons = np.linspace(min_lon, max_lon, steps_lon)\n",
    "    lats = np.linspace(min_lat, max_lat, steps_lat)\n",
    "    lon, lat = np.meshgrid(lons, lats)\n",
    "    coords = [[lon, lat] for lon, lat in zip(lon.flatten(), lat.flatten())]\n",
    "    return coords\n",
    "\n",
    "def plot_sampling_grid(coords, rect_width=0.025, output=False):\n",
    "    sampling_df = pd.DataFrame({\n",
    "        'lon': [coord[0] for coord in coords],\n",
    "        'lat': [coord[1] for coord in coords],\n",
    "        })\n",
    "    \n",
    "    features = []\n",
    "    for lon, lat in zip([coord[0] for coord in coords], [coord[1] for coord in coords]):\n",
    "        rect = box(lon - rect_width / 2, lat - rect_width / 2, lon + rect_width / 2, lat + rect_width / 2)\n",
    "        features.append(Feature(geometry=rect))\n",
    "        \n",
    "    feature_collection = FeatureCollection(features)\n",
    "    \n",
    "    map_config = {\"version\":\"v1\",\"config\":{\"visState\":{\"filters\":[],\"layers\":[{\"id\":\"ukfrcej\",\"type\":\"geojson\",\"config\":{\"dataId\":\"samples\",\"label\":\"samples\",\"color\":[137,218,193],\"columns\":{\"geojson\":\"geometry\"},\"isVisible\":True,\"visConfig\":{\"opacity\":0.8,\"strokeOpacity\":0.8,\"thickness\":1,\"strokeColor\":[210,0,0]}}}]},\"mapStyle\":{\"styleType\":\"satellite\"}}    }\n",
    "    \n",
    "    sampling_map = KeplerGl(height=600, \n",
    "                            data={'samples': geopandas.GeoDataFrame.from_features(feature_collection)},\n",
    "                            config=map_config)\n",
    "    return sampling_map\n",
    "\n",
    "def get_image_stack(coords, start_date='2020-05-01', rect_width=0.025, scale=10, num_months=1):\n",
    "    names = ['sample_' + str(i) for i in range(len(coords))]\n",
    "    history = get_history(coords, \n",
    "                          names,\n",
    "                          rect_width,\n",
    "                          start_date=start_date,\n",
    "                          num_months=num_months,\n",
    "                          #scale=rect_width * (100 / 0.025)\n",
    "                          scale=scale\n",
    "                         )\n",
    "    img_stack = create_img_stack(history)\n",
    "    print(\"Image shape before cropping:\", img_stack[0].shape)\n",
    "    min_dim = np.min([img.shape[:2] for img in img_stack])\n",
    "    img_stack = [img[:min_dim, :min_dim, :] for img in img_stack]\n",
    "    \n",
    "    return history, img_stack\n",
    "\n",
    "def predict_grid(model, history, img_stack, coords):\n",
    "    \n",
    "    preds = model.predict(normalize(img_stack))[:,1]\n",
    "    \n",
    "    cloud_free_coords = []\n",
    "    for site, coords in zip(history[start_date], coords):\n",
    "        if np.median(history[start_date][site]['B2']) > 0:\n",
    "            cloud_free_coords.append(coords)\n",
    "\n",
    "    preds_df = pd.DataFrame({\n",
    "        'pred': preds,\n",
    "        'lon': [coord[0] for coord in cloud_free_coords],\n",
    "        'lat': [coord[1] for coord in cloud_free_coords]}\n",
    "    )\n",
    "\n",
    "    return preds_df\n",
    "    \n",
    "def write_data(data_frame, file_path, rect_width):\n",
    "    data_frame.to_csv(file_path + '.csv', index=False)\n",
    "    \n",
    "    features = []\n",
    "    for lon, lat, pred in zip(list(data_frame['lon']), list(data_frame['lat']), list(data_frame['pred'])):\n",
    "        rect = box(lon - rect_width / 2, lat - rect_width / 2, lon + rect_width / 2, lat + rect_width / 2)\n",
    "        features.append(Feature(geometry=rect, properties={'pred': pred}))\n",
    "\n",
    "    feature_collection = FeatureCollection(features)\n",
    "    with open(file_path + '.geojson', 'w') as f:\n",
    "       dump(feature_collection, f)\n",
    "    \n",
    "    geopandas.GeoDataFrame.from_features(feature_collection).plot(column='pred', \n",
    "                                                                  cmap='seismic',\n",
    "                                                                  figsize=(10, 8),\n",
    "                                                                  vmin=0,\n",
    "                                                                  vmax=1)\n",
    "\n",
    "def stretch_histogram(array, min_val=0.1, max_val=0.75, gamma=1.2):\n",
    "    clipped = np.clip(array, min_val, max_val)\n",
    "    stretched = np.clip((clipped - min_val) / (max_val - min_val) ** gamma, 0, 1)\n",
    "    return stretched\n",
    "\n",
    "def sample_polygon(polygon_coords, num_samples, rect_width = 0.0075, min_intersection=0.25, plot=False):\n",
    "    site_polygon = Polygon(polygon_coords)\n",
    "    min_lon, min_lat, max_lon, max_lat = site_polygon.bounds\n",
    "    valid_points = []\n",
    "    \n",
    "    polygon_area = site_polygon.area\n",
    "    while len(valid_points) < num_samples:\n",
    "        rand_point = Point(np.random.uniform(min_lon - rect_width / 4, max_lon + rect_width / 4), \n",
    "                           np.random.uniform(min_lat - rect_width / 4, max_lat + rect_width / 4))\n",
    "        rect = box(rand_point.x - rect_width / 2, rand_point.y - rect_width / 2, rand_point.x + rect_width / 2, rand_point.y + rect_width / 2)\n",
    "        rect_area = rect.area\n",
    "        \n",
    "        if rect_area > polygon_area:\n",
    "            coverage_area = polygon_area\n",
    "        else:\n",
    "            coverage_area = rect.area\n",
    "            \n",
    "        if site_polygon.intersection(rect).area / coverage_area > 0.25:\n",
    "            valid_points.append(rand_point)\n",
    "        \n",
    "        if plot:\n",
    "            if site_polygon.intersection(rect).area / coverage_area > 0.25:\n",
    "                plt.plot(*rect.exterior.xy, c='C0', alpha=0.5)\n",
    "                plt.scatter(rand_point.x, rand_point.y, c='C0')\n",
    "            else:\n",
    "                plt.plot(*rect.exterior.xy, c='r', alpha=0.25)\n",
    "                plt.scatter(rand_point.x, rand_point.y, c='r')\n",
    "                #plt.title(f\"Polygon Overlap: {site_polygon.intersection(rect).area / site_polygon.area:.0%}\")\n",
    "    if plot:\n",
    "        plt.plot(*site_polygon.exterior.xy, c='k', linewidth=2)\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "    sample_points = [[point.x, point.y] for point in valid_points]\n",
    "    return sample_points\n",
    "\n",
    "def sample_geojson(geojson, num_samples, rect_width):\n",
    "    coords = []\n",
    "    names = []\n",
    "    for index, site in enumerate(geojson):\n",
    "        coords += sample_polygon(site['geometry']['coordinates'][0], num_samples, rect_width)\n",
    "        names += [f\"{index}_{site_num}\" for site_num in range(num_samples)]\n",
    "    print(len(coords), \"sampling sites generated\")\n",
    "    return coords, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_polygon(positive_sites[0]['geometry']['coordinates'][0], 3, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand Picked Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('..', 'data')\n",
    "\n",
    "with open(os.path.join(data_dir, 'MinesPos2018-2020Sentinel.geojson'), 'r') as f:\n",
    "    positive_sites = json.load(f)['features']\n",
    "    \n",
    "with open(os.path.join(data_dir, 'MinesNeg2018-2020Sentinel.geojson'), 'r') as f:\n",
    "    negative_sites = json.load(f)['features']\n",
    "\n",
    "with open(os.path.join(data_dir, 'MinesNeg_caleb_selection.geojson'), 'r') as f:\n",
    "    additional_negative_sites = json.load(f)['features']\n",
    "    \n",
    "    \n",
    "positive_coords, positive_names = sample_geojson(positive_sites, num_samples, rect_width)\n",
    "negative_coords, negative_names = sample_geojson(negative_sites, 10, rect_width)\n",
    "selected_negative_coords = [site['geometry']['coordinates'][0:2] for site in additional_negative_sites]\n",
    "selected_negative_names = ['addtional_negative_' + str(i) for i in range(len(additional_negative_sites))]\n",
    "\n",
    "negative_coords = negative_coords + selected_negative_coords\n",
    "negative_names = negative_names + selected_negative_names\n",
    "print()\n",
    "print(len(negative_coords), 'negative sites loaded')\n",
    "print(len(positive_coords), 'positive sites loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampling_grid(positive_coords, rect_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date='2019-03-01'\n",
    "positive_history, positive_img_stack = get_image_stack(positive_coords, \n",
    "                                     start_date=start_date, \n",
    "                                     rect_width=rect_width, \n",
    "                                     scale=scale,\n",
    "                                     num_months=3\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview a selection of images\n",
    "\n",
    "rgb_img = []\n",
    "for img in positive_img_stack[:100]:\n",
    "    rgb = np.stack((img[:,:,3],\n",
    "                    img[:,:,2],\n",
    "                    img[:,:,1]), axis=-1)\n",
    "    rgb = stretch_histogram(normalize(rgb), 0.1, 1)\n",
    "    rgb_img.append(rgb)\n",
    "    \n",
    "for img in rgb_img:\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = f\"../data/caroni_positive_84px_5x_polygon_sampling\"\n",
    "with open(file_path + '.pkl', 'wb') as f:\n",
    "    pickle.dump(positive_img_stack, f)\n",
    "    print(\"Wrote file to:\", file_path + '.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = f\"../data/tambopata_mine_{min_lon},{max_lon}_{min_lat},{max_lat}_{rect_width}_{scale}\"\n",
    "with open(file_path + '.pkl', 'wb') as f:\n",
    "    pickle.dump(img_stack, f)\n",
    "    print(\"Wrote file to:\", file_path + '.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date='2019-03-01'\n",
    "negative_history, negative_img_stack = get_image_stack(negative_coords, \n",
    "                                     start_date=start_date, \n",
    "                                     rect_width=rect_width, \n",
    "                                     scale=scale,\n",
    "                                     num_months=3\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Sample Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect_width = 0.005\n",
    "\n",
    "#tambopata whole\n",
    "min_lon, max_lon = -70.72, -69.8\n",
    "min_lat, max_lat = -13.2, -12.57\n",
    "\n",
    "# Caroni whole\n",
    "#min_lon, max_lon = -63.09674922312161, -62.18700279001181\n",
    "#min_lat, max_lat = 4.514923184841662, 6.511625537541098\n",
    "\n",
    "#min_lon, max_lon = -70.64, -70.4\n",
    "#min_lat, max_lat = -12.96, -13.07\n",
    "\n",
    "steps_lon = 2\n",
    "steps_lat = 1\n",
    "\n",
    "sampling_coords = create_sampling_grid(min_lon, max_lon, min_lat, max_lat, steps_lon, steps_lat)\n",
    "sampling_map = plot_sampling_grid(sampling_coords, rect_width)\n",
    "sampling_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date='2019-05-01'\n",
    "scale = 10\n",
    "history, img_stack = get_image_stack(sampling_coords, start_date=start_date, rect_width=rect_width, scale=scale)\n",
    "\n",
    "file_path = f\"../data/tambopata_mine_{min_lon},{max_lon}_{min_lat},{max_lat}_{rect_width}_{scale}\"\n",
    "with open(file_path + '.pkl', 'wb') as f:\n",
    "    pickle.dump(img_stack, f)\n",
    "    print(\"Wrote file to:\", file_path + '.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Bolivar V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "with open('../outputs/bolivar_10x.geojson', 'r') as f:\n",
    "    bolivar = geojson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolivar_preds = np.array([point['properties']['mean'] for point in bolivar[:]])\n",
    "bolivar_coords = np.array([np.mean(point['geometry']['coordinates'][0], axis=(0)) for point in bolivar[:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.95\n",
    "bolivar_positive_coords = bolivar_coords[bolivar_preds > threshold]\n",
    "print(len(bolivar_preds), \"total samples\")\n",
    "print(len(bolivar_positive_coords), \"samples above prediction threshold of\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date='2019-05-01'\n",
    "\n",
    "rect_width = 0.0076\n",
    "scale = 10\n",
    "history, img_stack = get_image_stack(bolivar_positive_coords[:500], \n",
    "                                     start_date=start_date, \n",
    "                                     rect_width=rect_width, \n",
    "                                     scale=scale,\n",
    "                                     num_months=2\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_stack_84 = [img[:84,:84,:] for img in img_stack]\n",
    "with open('../data/84_px_bolivar_bootstrap_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(img_stack_84, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = []\n",
    "for img in img_stack:\n",
    "    rgb = np.stack((img[:,:,3],\n",
    "                    img[:,:,2],\n",
    "                    img[:,:,1]), axis=-1)\n",
    "    rgb = stretch_histogram(normalize(rgb), 0.1, 1)\n",
    "    rgb_img.append(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in rgb_img:\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Bolivar V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../outputs/bolivar_10x_bootstrap_v1.geojson', 'r') as f:\n",
    "    bolivar = geojson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolivar_preds = np.array([point['properties']['mean'] for point in bolivar[:]])\n",
    "bolivar_coords = np.array([np.mean(point['geometry']['coordinates'][0], axis=(0)) for point in bolivar[:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "bolivar_positive_coords = bolivar_coords[bolivar_preds > threshold]\n",
    "print(len(bolivar_preds), \"total samples\")\n",
    "print(len(bolivar_positive_coords), \"samples above prediction threshold of\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date='2019-05-01'\n",
    "\n",
    "rect_width = 0.0075\n",
    "scale = 10\n",
    "history, img_stack = get_image_stack(bolivar_positive_coords, \n",
    "                                     start_date=start_date, \n",
    "                                     rect_width=rect_width, \n",
    "                                     scale=scale,\n",
    "                                     num_months=3\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/84_px_bolivar_bootstrap_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(img_stack, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = []\n",
    "for img in img_stack:\n",
    "    rgb = np.stack((img[:,:,3],\n",
    "                    img[:,:,2],\n",
    "                    img[:,:,1]), axis=-1)\n",
    "    rgb = stretch_histogram(normalize(rgb), 0.1, 1)\n",
    "    rgb_img.append(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in rgb_img[:100]:\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "inception = keras.models.load_model('../models/84_px_bootstrap_v2_polygon_5x_sample_1_base_sample_100_epochs_02-07-21.h5')\n",
    "preds = inception.predict(normalize(img_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_img = 15\n",
    "plt.figure(figsize=(16,16), dpi=150)\n",
    "for index, (img, pred) in enumerate(zip(rgb_img[:num_img ** 2], preds[:num_img ** 2, 1])):\n",
    "    plt.subplot(num_img, num_img, index + 1)\n",
    "    plt.imshow(np.clip(img, 0, 1))\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{pred:.2f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../outputs/bolivar_10x_inception_v1.geojson', 'r') as f:\n",
    "    bolivar = geojson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolivar_preds = np.array([point['properties']['mean'] for point in bolivar[:]])\n",
    "bolivar_coords = np.array([np.mean(point['geometry']['coordinates'][0], axis=(0)) for point in bolivar[:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "bolivar_positive_coords = bolivar_coords[bolivar_preds > threshold]\n",
    "print(len(bolivar_preds), \"total samples\")\n",
    "print(len(bolivar_positive_coords), \"samples above prediction threshold of\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_index = np.random.choice(len(bolivar_positive_coords), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date='2019-03-01'\n",
    "\n",
    "rect_width = 0.0075\n",
    "scale = 10\n",
    "inception_history, inception_img_stack = get_image_stack(bolivar_positive_coords[bootstrap_index], \n",
    "                                     start_date=start_date, \n",
    "                                     rect_width=rect_width, \n",
    "                                     scale=scale,\n",
    "                                     num_months=3\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_rgb_img = []\n",
    "for img in inception_img_stack:\n",
    "    rgb = np.stack((img[:,:,3],\n",
    "                    img[:,:,2],\n",
    "                    img[:,:,1]), axis=-1)\n",
    "    rgb = stretch_histogram(normalize(rgb), 0.1, 1)\n",
    "    inception_rgb_img.append(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inception_rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in inception_rgb_img[:100]:\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/84_px_bolivar_inception_bootstrap_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(inception_img_stack, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense River 84px patch sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_river_samples = pd.read_csv(os.path.join(data_dir, 'dense_river_0.01.csv'))\n",
    "dense_river_coords = [[lon, lat] for lon, lat in zip(dense_river_samples['lon'], dense_river_samples['lat'])]\n",
    "start_date='2019-03-01'\n",
    "\n",
    "rect_width = 0.0075\n",
    "scale = 10\n",
    "river_history, river_img_stack = get_image_stack(dense_river_coords, \n",
    "                                     start_date=start_date, \n",
    "                                     rect_width=rect_width, \n",
    "                                     scale=scale,\n",
    "                                     num_months=2\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_rgb_img = []\n",
    "for img in river_img_stack:\n",
    "    rgb = np.stack((img[:,:,3],\n",
    "                    img[:,:,2],\n",
    "                    img[:,:,1]), axis=-1)\n",
    "    rgb = stretch_histogram(normalize(rgb), 0.1, 1)\n",
    "    river_rgb_img.append(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in river_rgb_img[:100]:\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/84_px_inception_v4_02-08-21.h5')\n",
    "preds = model.predict(normalize(river_img_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'preds': preds[:,1], \n",
    "    'lon': [coord[0] for coord in dense_river_coords],\n",
    "    'lat': [coord[1] for coord in dense_river_coords]}).to_csv('../outputs/dense_river_84px_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
