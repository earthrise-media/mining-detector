{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for model training\n",
    "\n",
    "Trains streaming pre-downloaded raster data on disk. Expected data folder structure:\n",
    "\n",
    "```\n",
    "training_patches/\n",
    "├── train/\n",
    "│   ├── 0/\n",
    "│   └── 1/\n",
    "└── val/\n",
    "    ├── 0/\n",
    "    └── 1/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T22:07:01.749808Z",
     "start_time": "2025-10-15T22:06:56.436658Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "parent_dir = os.path.split(os.getcwd())[0]\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "import model_library\n",
    "\n",
    "WORK_DIR = '..'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T15:15:19.509938Z",
     "start_time": "2025-10-14T15:15:19.356544Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU functionality sanity check - should run fast\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "x = tf.random.normal([4096, 4096])\n",
    "y = tf.matmul(x, x) \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation / augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF / In-RAM version - current preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T22:07:01.795731Z",
     "start_time": "2025-10-15T22:07:01.753102Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 1️⃣ Define augmentation pipeline (reuse a single instance)\n",
    "# ----------------------------\n",
    "def get_satellite_augmentation_pipeline():\n",
    "    return keras.Sequential([\n",
    "        keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        keras.layers.RandomRotation(factor=1.0, fill_mode=\"reflect\"),  # full 360°\n",
    "        keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1, fill_mode=\"reflect\"),\n",
    "        keras.layers.RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1), fill_mode=\"reflect\"),\n",
    "        keras.layers.RandomContrast(factor=0.1),\n",
    "        keras.layers.GaussianNoise(stddev=0.01)\n",
    "    ])\n",
    "\n",
    "aug_pipeline = get_satellite_augmentation_pipeline()\n",
    "\n",
    "def load_dataset(data_dir, bands_to_use=None):\n",
    "    \"\"\"\n",
    "    Loads all images from '0' and '1' subdirectories into RAM.\n",
    "\n",
    "    Returns:\n",
    "        X: np.ndarray of shape (num_samples, H, W, C)\n",
    "        y: np.ndarray of shape (num_samples,)\n",
    "    \"\"\"\n",
    "    files_class_0 = glob.glob(os.path.join(data_dir, '0', '*.tif'))\n",
    "    files_class_1 = glob.glob(os.path.join(data_dir, '1', '*.tif'))\n",
    "    files = files_class_0 + files_class_1\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .tif files found in '0' or '1' subdirectories of {data_dir}\")\n",
    "\n",
    "    imgs, labels = [], []\n",
    "\n",
    "    for file_path in files:\n",
    "        import rasterio\n",
    "        with rasterio.open(file_path) as src:\n",
    "            arr = src.read()  # (bands, H, W)\n",
    "            if bands_to_use is not None:\n",
    "                arr = arr[bands_to_use, :, :]\n",
    "            arr = np.moveaxis(arr, 0, -1)  # (H, W, C)\n",
    "            arr = arr.astype(np.float32) / 10000.0\n",
    "            imgs.append(arr)\n",
    "\n",
    "        label_str = os.path.basename(os.path.dirname(file_path))\n",
    "        labels.append(int(label_str))\n",
    "\n",
    "    X = np.stack(imgs, axis=0)\n",
    "    y = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# ----------------------------\n",
    "# 2️⃣ Dataset creation function\n",
    "# ----------------------------\n",
    "def make_tf_dataset(X, y, batch_size=8, shuffle=True, augment=True):\n",
    "    \"\"\"\n",
    "    X, y: NumPy arrays \n",
    "    batch_size: int\n",
    "    shuffle: whether to shuffle dataset\n",
    "    augment: whether to apply augmentation\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(X))\n",
    "\n",
    "    if augment:\n",
    "        # Apply augmentation on CPU to avoid GPU memory spikes\n",
    "        def augment_on_cpu(x, y):\n",
    "            with tf.device(\"/CPU:0\"):\n",
    "                x_aug = aug_pipeline(x)\n",
    "            return x_aug, y\n",
    "\n",
    "        dataset = dataset.map(augment_on_cpu, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=8)  # small buffer reduces GPU memory spikes\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pattern:\n",
    "\n",
    "```\n",
    "X_train, y_train = load_dataset(os.path.join(data_dir, 'train'))\n",
    "X_val, y_val = load_dataset(os.path.join(data_dir, 'val'))\n",
    "\n",
    "train_ds = make_tf_dataset(X_train, y_train, batch_size=batch_size, augment=True, shuffle=True)\n",
    "val_ds = make_tf_dataset(X_val, y_val, batch_size=batch_size, augment=False, shuffle=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T22:07:01.818217Z",
     "start_time": "2025-10-15T22:07:01.797350Z"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (48, 48, 13)\n",
    "print(\"Input Shape:\", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T15:16:46.670579Z",
     "start_time": "2025-10-14T15:16:46.288437Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model_library.ResNet18(input_shape=input_shape)\n",
    "model_name = '48px_v1.1_ResNet18_2025-10-14'\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(1e-3), \n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "    metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
    "    run_eagerly=True\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T22:07:04.320250Z",
     "start_time": "2025-10-15T22:07:04.048372Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(WORK_DIR, 'data/training_patches2025-09-26T15:38')\n",
    "\n",
    "positive_paths =  glob.glob(f\"{data_dir}/train/1/*.tif\")\n",
    "negative_paths = glob.glob(f\"{data_dir}/train/0/*.tif\")\n",
    "pos_val_paths = glob.glob(f\"{data_dir}/val/1/*.tif\")\n",
    "neg_val_paths = glob.glob(f\"{data_dir}/val/0/*.tif\")\n",
    "print(f\"{len(positive_paths)} train positives\")\n",
    "print(f\"{len(negative_paths)} train negatives\")\n",
    "print(f\"{len(pos_val_paths)} val positives\")\n",
    "print(f\"{len(neg_val_paths)} val negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T22:08:08.598171Z",
     "start_time": "2025-10-15T22:07:05.057363Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_dataset(os.path.join(data_dir, 'train'))\n",
    "X_val, y_val = load_dataset(os.path.join(data_dir, 'val'))\n",
    "\n",
    "batch_size = 8\n",
    "train_ds = make_tf_dataset(X_train, y_train, batch_size=batch_size, augment=True, shuffle=True)\n",
    "val_ds = make_tf_dataset(X_val, y_val, batch_size=batch_size, augment=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T22:08:19.408040Z",
     "start_time": "2025-10-15T22:08:18.362336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload a model for further training\n",
    "\n",
    "model_name = 'ResNet1820250829_131606'\n",
    "model = keras.models.load_model(os.path.join(WORK_DIR, f'checkpoints/{model_name}.h5'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adam(3e-5), \n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "    metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
    "    run_eagerly=True   # Required w/ GPU on Mac\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T22:08:33.375209Z",
     "start_time": "2025-10-15T22:08:33.328372Z"
    }
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_dir = os.path.join(WORK_DIR, \"checkpoints\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "try: \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}{timestamp}.h5\")\n",
    "except NameError: \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"best_model{timestamp}.h5\")\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor=\"val_acc\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_acc\",\n",
    "    patience=40,\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_cb = ReduceLROnPlateau(\n",
    "    monitor=\"val_acc\",\n",
    "    factor=0.33,\n",
    "    patience=20,\n",
    "    min_delta=0.005,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:08:15.997486Z",
     "start_time": "2025-10-15T22:08:37.695263Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=100, \n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_cb, reduce_lr_cb]#, earlystop_cb,]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:08:34.082375Z",
     "start_time": "2025-10-16T08:08:33.749943Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch = 171\n",
    "resolution = 48\n",
    "version_number = 'v1.1_ResNet18_2025-10-14'\n",
    "current_date = date.today()\n",
    "model_path = os.path.join(WORK_DIR, f\"checkpts-tmp/{resolution}px_v{version_number}ep{epoch}_{current_date.isoformat()}.h5\")\n",
    "\n",
    "assert not os.path.exists(model_path), f\"Model {model_path} already exists\"\n",
    "\n",
    "model.save(model_path)\n",
    "print(f\"Saved {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the threshold that maximizes performance on the test set. Note that while this may be the optimum performance on the test set, it does not account for the fact that false positives are functionally worse than false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:24:56.583232Z",
     "start_time": "2025-10-16T08:24:55.832996Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = '48px_v1.1_ResNet18ep171_2025-10-16'\n",
    "model = keras.models.load_model(os.path.join(WORK_DIR, f'checkpts-tmp/{model_name}.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T08:51:52.018508Z",
     "start_time": "2025-10-14T08:51:51.945692Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:12:06.205272Z",
     "start_time": "2025-10-16T08:11:53.257200Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Assuming S2L1C 13 band input imagery\n",
    "bands_to_use = list(range(13))  \n",
    "if model.input_shape[-1] == 12:\n",
    "    bands_to_use.remove(10)  # drop B10 for old models\n",
    "\n",
    "X_val, y_val = load_dataset(os.path.join(data_dir, 'val'), bands_to_use=bands_to_use)\n",
    "val_ds = make_tf_dataset(X_val, y_val, batch_size=batch_size, augment=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T10:00:56.847746Z",
     "start_time": "2025-10-16T10:00:47.542896Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    preds = model.predict(val_ds, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:25:07.979799Z",
     "start_time": "2025-10-16T08:25:07.959271Z"
    }
   },
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:25:08.000987Z",
     "start_time": "2025-10-16T08:25:07.980587Z"
    }
   },
   "outputs": [],
   "source": [
    "# New models\n",
    "preds = preds.squeeze()\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:22:36.436622Z",
     "start_time": "2025-10-16T08:22:36.416857Z"
    }
   },
   "outputs": [],
   "source": [
    "# For the old ensemble\n",
    "#print(preds.shape)\n",
    "#preds = np.mean(preds, axis=1)\n",
    "#preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:22:36.517606Z",
     "start_time": "2025-10-16T08:22:36.437471Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_curve(preds, y_true, thresholds=np.arange(.01, 1.01, .01)):\n",
    "    \"\"\"Compute accuracy curve as function of threshold\"\"\"\n",
    "    score = [np.sum((preds >= t).astype('int') == y_true) / len(y_true) for t in thresholds]\n",
    "    plt.plot(thresholds, score)\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.title(f\"Optimal Threshold: {thresholds[np.argmax(score)]:.2f} w/ accuracy {score[np.argmax(score)]:.2f}\")\n",
    "\n",
    "acc_curve(preds, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:22:36.675138Z",
     "start_time": "2025-10-16T08:22:36.518785Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1_curve(preds, y_true, thresholds=np.arange(.01, 1.01, .01)):\n",
    "    \"\"\"Compute F1 curve.\"\"\"\n",
    "    f1s = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (preds >= t)\n",
    "        f1s.append(f1_score(y_true, y_pred))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(thresholds, f1s, label='Patchwise')\n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('F1 score')\n",
    "    ax.legend(loc='lower left')\n",
    "    plt.title(f\"Optimal Threshold: {thresholds[np.argmax(f1s)]:.2f} w/ F1 {f1s[np.argmax(f1s)]:.2f}\")\n",
    "    return fig, ax\n",
    "\n",
    "f1_curve(preds, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:54:19.057318Z",
     "start_time": "2025-10-16T09:54:18.965063Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "report = classification_report(y_val, preds > threshold, target_names=['No Mine', 'Mine'], output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:14:52.417548Z",
     "start_time": "2025-10-16T08:14:52.360866Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "target_names = ['No Mine', 'Mine']\n",
    "training_dataset = 'collected_locations2025-09-26T15:38.geojson'\n",
    "\n",
    "model_path = os.path.join(WORK_DIR, f'checkpts-tmp/{model_name}.h5')\n",
    "with open(model_path.split('.h5')[0] + f\"_config-t{threshold}.txt\", 'w') as f:\n",
    "    f.write(f'Training dataset: {training_dataset}')\n",
    "    f.write(f\"\\nBatch Size: {batch_size}\")\n",
    "    f.write(f'\\n\\nClassification Report at {threshold}\\n')\n",
    "    f.write(classification_report(y_val, preds > threshold, target_names=target_names))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot images that the model classifies incorrectly. Can be useful to evaluate model bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T06:49:47.300452Z",
     "start_time": "2025-08-14T06:49:36.402488Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "test_model = model\n",
    "val_images = x_test\n",
    "val_labels = y_test\n",
    "test_labels = val_labels\n",
    "test_preds = test_model.predict(val_images)\n",
    "for index, (label, pred, img) in enumerate(zip(test_labels, test_preds, val_images)):\n",
    "    pred = pred[0]\n",
    "    if pred < threshold:\n",
    "        binary_pred = 0\n",
    "    else:\n",
    "        binary_pred = 1\n",
    "    if label != binary_pred:\n",
    "        rgb = (img[:,:,3:0:-1] * 10000 / 3000)\n",
    "        fig = plt.figure(figsize=(2,2), facecolor=(1,1,1), dpi=150)\n",
    "        plt.imshow(np.clip(rgb, 0, 1))\n",
    "        plt.title(f\"label: {label} - pred: {pred:.2f}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bec97cbb607180795486aa419a93884fe3d0b55501c3e5098d64200fe61c3ffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
