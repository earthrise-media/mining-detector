{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import ee\n",
    "import geopandas as gpd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from descarteslabs.geo import DLTile\n",
    "from tqdm import tqdm\n",
    "\n",
    "ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tiles(region, tilesize, padding):\n",
    "    tiles = DLTile.iter_from_shape(region, tilesize=tilesize, resolution=10, pad=padding)\n",
    "    tiles = [tile for tile in tiles]\n",
    "    return tiles\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "def get_image_data(tiles, start_date, end_date, clear_threshold=0.6):\n",
    "    # Harmonized Sentinel-2 Level 2A collection.\n",
    "    s2 = ee.ImageCollection('COPERNICUS/S2_HARMONIZED')\n",
    "\n",
    "    # Cloud Score+ image collection. Note Cloud Score+ is produced from Sentinel-2\n",
    "    # Level 1C data and can be applied to either L1C or L2A collections.\n",
    "    csPlus = ee.ImageCollection('GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED')\n",
    "    QA_BAND = 'cs_cdf'\n",
    "\n",
    "    # Make a clear median composite.\n",
    "    composite = (s2\n",
    "        .filterDate(start_date, end_date)\n",
    "        .linkCollection(csPlus, [QA_BAND])\n",
    "        .map(lambda img: img.updateMask(img.select(QA_BAND).gte(clear_threshold)))\n",
    "        .median())\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Process each tile in parallel\n",
    "        futures = [executor.submit(process_tile, tile, composite) for tile in tiles]\n",
    "        \n",
    "        # Collect the results as they become available\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            data.append(result)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_tile(tile, composite):\n",
    "    tile_geom = ee.Geometry.Rectangle(tile.geometry.bounds)\n",
    "    composite_tile = composite.clipToBoundsAndScale(\n",
    "        geometry=tile_geom,\n",
    "        width=tile.tilesize + 2,\n",
    "        height=tile.tilesize + 2)\n",
    "    npy = ee.data.computePixels({\n",
    "        'bandIds': ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8A', 'B8', 'B9', 'B11', 'B12'],\n",
    "        'expression': composite_tile,\n",
    "        'fileFormat': 'NUMPY_NDARRAY',\n",
    "        'grid': {'crsCode': tile.crs},\n",
    "    })\n",
    "    \n",
    "    return npy\n",
    "\n",
    "def pad_patch(patch, height, width=None):\n",
    "    \"\"\"\n",
    "    Depending on how a polygon falls across pixel boundaries, the resulting patch can be slightly\n",
    "    bigger or smaller than intended.\n",
    "    pad_patch trims pixels extending beyond the desired number of pixels if the\n",
    "    patch is larger than desired. If the patch is smaller, it will fill the\n",
    "    edge by reflecting the values.\n",
    "    If trimmed, the patch should be trimmed from the center of the patch.\n",
    "    \"\"\"\n",
    "    if width is None:\n",
    "        width = height\n",
    "    \n",
    "    # convert from a structured array to a numpy array\n",
    "    #patch_dtypes = patch.dtype\n",
    "    patch = np.array(patch.tolist())\n",
    "    patch_height, patch_width,_ = patch.shape\n",
    "    \n",
    "    if patch_height > height:\n",
    "        trim_top = (patch_height - height) // 2\n",
    "        trim_bottom = trim_top + height\n",
    "    else:\n",
    "        trim_top = 0\n",
    "        trim_bottom = patch_height\n",
    "    \n",
    "    if patch_width > width:\n",
    "        trim_left = (patch_width - width) // 2\n",
    "        trim_right = trim_left + width\n",
    "    else:\n",
    "        trim_left = 0\n",
    "        trim_right = patch_width\n",
    "    \n",
    "    trimmed_patch = patch[trim_top:trim_bottom, trim_left:trim_right, :]\n",
    "    \n",
    "    if patch_height < height:\n",
    "        pad_top = (height - patch_height) // 2\n",
    "        pad_bottom = pad_top + patch_height\n",
    "        padded_patch = np.pad(trimmed_patch, ((pad_top, height - pad_bottom), (0, 0), (0, 0)), mode='reflect')\n",
    "    else:\n",
    "        padded_patch = trimmed_patch\n",
    "    \n",
    "    if patch_width < width:\n",
    "        pad_left = (width - patch_width) // 2\n",
    "        pad_right = pad_left + patch_width\n",
    "        padded_patch = np.pad(padded_patch, ((0, 0), (pad_left, width - pad_right), (0, 0)), mode='reflect')\n",
    "    # convert back to a structured array where each band is named\n",
    "    #padded_patch = np.array(padded_patch, dtype=patch_dtypes)\n",
    "    return padded_patch\n",
    "\n",
    "def chips_from_tile(data, tile, width, stride):\n",
    "    \"\"\"\n",
    "    Break a larger tile of Sentinel data into a set of patches that\n",
    "    a model can process.\n",
    "    Inputs:\n",
    "        - data: Sentinel data. Typically a numpy masked array\n",
    "        - tile_coords: bounds of the tile in the format (west, south, east, north)\n",
    "        - stride: number of pixels between each patch\n",
    "    Outputs:\n",
    "        - chips: A list of numpy arrays of the shape the model requires\n",
    "        - chip_coords: A list of shapely polygon features describing the patch bounds\n",
    "    \"\"\"\n",
    "    (west, south, east, north) = tile.bounds\n",
    "    delta_x = east - west\n",
    "    delta_y = south - north\n",
    "    x_per_pixel = delta_x / np.shape(data)[0]\n",
    "    y_per_pixel = delta_y / np.shape(data)[1]\n",
    "\n",
    "    # The tile is broken into the number of whole patches\n",
    "    # Regions extending beyond will not be padded and processed\n",
    "    chip_coords = []\n",
    "    chips = []\n",
    "\n",
    "    # Extract patches and create a shapely polygon for each patch\n",
    "    for i in range(0, np.shape(data)[0] - width + stride, stride):\n",
    "        for j in range(0, np.shape(data)[1] - width + stride, stride):\n",
    "            patch = data[j : j + width, i : i + width]\n",
    "            chips.append(patch)\n",
    "\n",
    "            nw_coord = [west + i * x_per_pixel, north + j * y_per_pixel]\n",
    "            ne_coord = [west + (i + width) * x_per_pixel, north + j * y_per_pixel]\n",
    "            sw_coord = [west + i * x_per_pixel, north + (j + width) * y_per_pixel]\n",
    "            se_coord = [west + (i + width) * x_per_pixel, north + (j + width) * y_per_pixel]\n",
    "            tile_geometry = [nw_coord, sw_coord, se_coord, ne_coord, nw_coord]\n",
    "            chip_coords.append(shapely.geometry.Polygon(tile_geometry))\n",
    "    chip_coords = gpd.GeoDataFrame(geometry=chip_coords, crs=tile.crs)\n",
    "    return chips, chip_coords\n",
    "\n",
    "def unit_norm(samples):\n",
    "    \"\"\"\n",
    "    Channel-wise normalization of pixels in a patch.\n",
    "    Means and deviations are constants generated from an earlier dataset.\n",
    "    If changed, models will need to be retrained\n",
    "    Input: (n,n,12) numpy array or list.\n",
    "    Returns: normalized numpy array\n",
    "    \"\"\"\n",
    "    means = [1405.8951, 1175.9235, 1172.4902, 1091.9574, 1321.1304, 2181.5363, 2670.2361, 2491.2354, 2948.3846, 420.1552, 2028.0025, 1076.2417]\n",
    "    deviations = [291.9438, 398.5558, 504.557, 748.6153, 651.8549, 730.9811, 913.6062, 893.9428, 1055.297, 225.2153, 970.1915, 752.8637]\n",
    "    normalized_samples = np.zeros_like(samples).astype('float32')\n",
    "    for i in range(0, 12):\n",
    "        #normalize each channel to global unit norm\n",
    "        normalized_samples[:,:,i] = (np.array(samples.astype(float))[:,:,i] - means[i]) / deviations[i]\n",
    "    return normalized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/44px_v2.9_2022-02-28.h5')\n",
    "\n",
    "region = gpd.read_file('../data/boundaries/test_region.geojson').geometry[0].__geo_interface__\n",
    "\n",
    "tile_size = 528\n",
    "tile_padding = 16\n",
    "\n",
    "chip_width = 44\n",
    "chip_stride = 11\n",
    "\n",
    "start_date = datetime(2023, 5, 1)\n",
    "end_date = datetime(2023, 11, 1)\n",
    "clear_threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 tiles have been created\n",
      "Tiles have been broken into 99,225 chips\n",
      "3101/3101 [==============================] - 7s 2ms/step\n",
      "136,604 hectares were analyzed in 1 minutes and 18 seconds\n",
      "At this speed, you could process an area the size of Rhode Island in 180 seconds\n",
      "and the Amazon basin in 110.3 hours (4.6 days)\n"
     ]
    }
   ],
   "source": [
    "# start a timer\n",
    "start = datetime.now()\n",
    "tiles = create_tiles(region, tile_size, tile_padding)\n",
    "print(f\"{len(tiles)} tiles have been created\")\n",
    "\n",
    "data = get_image_data(tiles, start_date, end_date, clear_threshold)\n",
    "data = np.array([pad_patch(patch, tile_size) for patch in data])\n",
    "data_norm = np.array([unit_norm(d) for d in data])\n",
    "\n",
    "# convert the tiles to chips\n",
    "chip_stack = []\n",
    "chip_geom_stack = []\n",
    "for tile_data, tile_geom in zip(data_norm, tiles):\n",
    "    chips, chip_geoms = chips_from_tile(tile_data, tile_geom, chip_width, chip_stride)\n",
    "    chip_stack.extend(chips)\n",
    "    chip_geom_stack.append(chip_geoms)\n",
    "\n",
    "chip_stack = np.array(chip_stack)\n",
    "chip_geom_stack = gpd.GeoDataFrame(pd.concat(chip_geom_stack, ignore_index=True), crs=tiles[0].crs)\n",
    "print(f\"Tiles have been broken into {len(chip_stack):,.0f} chips\")\n",
    "\n",
    "# make predictions and create a gdf\n",
    "preds = model.predict(chip_stack)[:,1]\n",
    "pred_df = gpd.GeoDataFrame(chip_geom_stack, crs=tiles[0].crs)\n",
    "pred_df['preds'] = preds\n",
    "\n",
    "# end the timer\n",
    "end = datetime.now()\n",
    "\n",
    "# print the time it took to run the pipeline\n",
    "area_m2 = len(tiles) * (tile_size * 10) ** 2\n",
    "# convert the meters squared to hectares\n",
    "area_ha = area_m2 / 10000\n",
    "duration = end - start\n",
    "minutes, seconds = divmod(duration.total_seconds(), 60)\n",
    "print(f\"{area_ha:,.0f} hectares were analyzed in {minutes:.0f} minutes and {seconds:.0f} seconds\")\n",
    "print(f\"At this speed, you could process an area the size of Rhode Island in {313900 * duration.total_seconds() / area_ha:.0f} seconds\")\n",
    "minutes, seconds = divmod(2203 * 313900 * duration.total_seconds() / area_ha, 60)\n",
    "print(f\"and the Amazon basin in {minutes / 60:,.1f} hours ({minutes / 60 / 24:,.1f} days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the chips in chip_stack where the preds are above a threshold\n",
    "threshold = 0.9\n",
    "positive_indices = np.where(preds > threshold)[0]\n",
    "positive_chips = chip_stack[positive_indices]\n",
    "print(len(positive_chips), 'chips with predictions above', threshold)\n",
    "num_plots = int(np.ceil(np.sqrt(len(positive_chips))))\n",
    "fig, axs = plt.subplots(num_plots, num_plots, figsize=(10,10))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    if i < len(positive_chips):\n",
    "        ax.imshow(np.clip((positive_chips[i][:,:,(3,2,1)] + 2) / 5, 0, 1))\n",
    "        ax.set_axis_off()\n",
    "    else:\n",
    "        ax.set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
